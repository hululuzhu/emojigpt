{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhaQfXn38zIQRroGlHheVI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hululuzhu/emojigpt/blob/main/emojigpt_inference_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FXO-ENDm-2r"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes > /dev/null\n",
        "!pip install -q peft > /dev/null\n",
        "!pip install -q transformers > /dev/null\n",
        "!pip install -q sentencepiece > /dev/null\n",
        "\n",
        "from peft import PeftModel\n",
        "import transformers\n",
        "import torch\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "\n",
        "model_name_or_path = \"daryl149/llama-2-7b-chat-hf\"\n",
        "tokenizer_name_or_path = \"daryl149/llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(tokenizer_name_or_path)\n",
        "tokenizer.pad_token_id = 0\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "original_8bit_llama_model = LlamaForCausalLM.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True)\n",
        "\n",
        "\n",
        "emoji_model = PeftModel.from_pretrained(\n",
        "    original_8bit_llama_model, \"hululuzhu/llama2-chat-emoji-lora\")\n",
        "\n",
        "CUTOFF_LEN = 48\n",
        "def tokenize(tokenizer, prompt, cutoff_len, add_eos_token=True):\n",
        "  result = tokenizer(\n",
        "      prompt,\n",
        "      truncation=True,\n",
        "      max_length=cutoff_len,\n",
        "      padding=False,\n",
        "      return_tensors=None,\n",
        "  )\n",
        "  if (\n",
        "      result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
        "      and len(result[\"input_ids\"]) < cutoff_len\n",
        "      and add_eos_token\n",
        "  ):\n",
        "    result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
        "    result[\"attention_mask\"].append(1)\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "  return result\n",
        "\n",
        "def answer(p_in, my_model=emoji_model):\n",
        "  batch = tokenizer(\n",
        "      p_in,\n",
        "      return_tensors='pt',\n",
        "  )\n",
        "  with torch.cuda.amp.autocast(): # required for mixed precisions\n",
        "    output_tokens = my_model.generate(\n",
        "        **batch, max_new_tokens=batch['input_ids'].shape[-1])\n",
        "  # print(output_tokens[0])\n",
        "  out = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "  # My own post-processing logic to \"cheat\" to align chars\n",
        "  if len(out) > len(p_in) * 2 - 7:\n",
        "    out = out[:len(p_in) * 2 - 7 - len(out)] # perfectly match chars\n",
        "  # replace the last N for visibility\n",
        "  if out.count('\\n') > 1:\n",
        "    out = out[::-1].replace(\"\\n\", \"n\\\\\", 1)[::-1]\n",
        "  # if out.startswith(p_in):\n",
        "  #   out = out[len(p_in):]\n",
        "  print(out)\n",
        "  print()\n",
        "\n",
        "answer(\"What can I surprise my wife?\")\n",
        "answer(\"What can I surprise my kids?\")\n",
        "answer(\"What can I surprise my parents?\")\n",
        "answer(\"What can I surprise my friends?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your custom questions\n",
        "answer(\"Why do kids love summer?\")"
      ],
      "metadata": {
        "id": "cXp-AjquVRVF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}