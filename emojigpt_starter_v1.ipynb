{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVJPpn5TjG7qW293VLDi6c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hululuzhu/emojigpt/blob/main/emojigpt_starter_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An end-to-end notebook to train a fun EmojiGPT AI\n",
        "- Please set your environment to T4 + high_ram\n",
        "- We used an unofficial llama2 checkpoint published at huggingface to demo purpose, ideally you want to go through [the official process](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n",
        "- You may have already collect data like [this set](https://huggingface.co/datasets/hululuzhu/silly-emoji-qa)\n",
        "  - Why do we have different seasons?\t🌍☀️🔄\n",
        "  - How do fish breathe underwater?\t🐟💦💨"
      ],
      "metadata": {
        "id": "BVXJ7IqgJmAB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YdDtKHRDJi-H"
      },
      "outputs": [],
      "source": [
        "# @title Install packages and imports, load llama model\n",
        "print(\"Installing and loading, taking about 5 mins, please be patient\")\n",
        "\n",
        "!pip install -q bitsandbytes > /dev/null\n",
        "!pip install -q datasets loralib sentencepiece > /dev/null\n",
        "!pip install -q peft > /dev/null\n",
        "!pip install -q transformers > /dev/null\n",
        "\n",
        "from datasets import Dataset, load_dataset\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "from peft import PeftModel, get_peft_config, get_peft_model, LoraConfig, TaskType, prepare_model_for_int8_training\n",
        "import pickle\n",
        "import sys\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, AutoModelForSeq2SeqLM, DataCollatorForLanguageModeling\n",
        "\n",
        "# TODO: Replace this unofficial path to official path\n",
        "def llama_model_tokenizer(llama_path='daryl149/llama-2-7b-chat-hf'):\n",
        "  llama = LlamaForCausalLM.from_pretrained(\n",
        "    llama_path,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True)\n",
        "  tokenizer = LlamaTokenizer.from_pretrained(llama_path)\n",
        "  tokenizer.pad_token_id = 0\n",
        "  tokenizer.padding_side = \"left\"\n",
        "  return llama, tokenizer\n",
        "\n",
        "llama, tokenizer = llama_model_tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the following line to take a look at llama model architecture\n",
        "# llama"
      ],
      "metadata": {
        "id": "anKvkQlKM6Yd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Support functions\n",
        "\n",
        "# Supress warning\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Why 48? This is the cap of total tokens in the small dataset, change if needed\n",
        "MAX_LEN = 48  #@param\n",
        "\n",
        "# Copied from Alpaca-LoRA, notice input_ids, attention_mask, and labels are\n",
        "# default expected columns in huggingface dataset lib\n",
        "def tokenize(tokenizer, prompt, cutoff_len=MAX_LEN, add_eos_token=True):\n",
        "  # there's probably a way to do this with the tokenizer settings\n",
        "  # but again, gotta move fast\n",
        "  result = tokenizer(\n",
        "      prompt,\n",
        "      truncation=True,\n",
        "      max_length=cutoff_len,\n",
        "      padding=False,\n",
        "      return_tensors=None,\n",
        "  )\n",
        "  if (\n",
        "      result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
        "      and len(result[\"input_ids\"]) < cutoff_len\n",
        "      and add_eos_token\n",
        "  ):\n",
        "    result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
        "    result[\"attention_mask\"].append(1)\n",
        "\n",
        "  # result[\"labels\"] = copy.deepcopy(result[\"input_ids\"])\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "  return result\n",
        "\n",
        "KIDS_QS = \"\"\"Why do we need to wear a seat belt in the car?\n",
        "How do bees make honey?\n",
        "Why do we have different seasons?\n",
        "How do fish breathe underwater?\n",
        "Why do we need to eat fruits and vegetables?\n",
        "How do birds fly?\n",
        "Why do we need to brush our teeth?\n",
        "How do plants grow from seeds?\n",
        "Why do we need to wear sunscreen in the sun?\n",
        "How do butterflies transform from caterpillars?\"\"\".split('\\n')\n",
        "\n",
        "def eval_model(my_model):\n",
        "  for p_in in KIDS_QS:\n",
        "    batch = tokenizer(\n",
        "        p_in,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "    with torch.cuda.amp.autocast(): # required for mixed precisions\n",
        "      output_tokens = my_model.generate(\n",
        "          **batch, max_new_tokens=batch['input_ids'].shape[-1])\n",
        "    # print(output_tokens[0])\n",
        "    out = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "    # My own post-processing logic to \"cheat\" to align chars\n",
        "    if len(out) > len(p_in) * 2 - 7:\n",
        "      out = out[:len(p_in) * 2 - 7 - len(out)] # perfectly match chars\n",
        "    # replace the last N for visibility\n",
        "    if out.count('\\n') > 1:\n",
        "      out = out[::-1].replace(\"\\n\", \"n\\\\\", 1)[::-1]\n",
        "    if out.startswith(p_in):\n",
        "      out = out[len(p_in):]\n",
        "    print(p_in, out)\n",
        "    print()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CG1KeJiELq67"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title How does the llama model work out of box?\n",
        "print(\"Please note we cap the number of output to speed up, so answers look like cut.\\n\")\n",
        "eval_model(llama)"
      ],
      "metadata": {
        "id": "jo8sCsiHNGGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Time to prepare the model to train!\n",
        "print(\"We also used an efficient training trick called LoRA\")\n",
        "llama = prepare_model_for_int8_training(llama)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32, # scaling param related to r, reuse alpaca-lora\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "llama = get_peft_model(llama, config)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Nw2vBOOrOE-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Now let's load the data\n",
        "print(\"If you have your own CSV/TSV data, Change to your own path\")\n",
        "print(\"You can even upload your CSV/TSV to Colab, and use relative path\")\n",
        "\n",
        "CSV_PATH = 'https://huggingface.co/datasets/hululuzhu/silly-emoji-qa/resolve/main/train.csv'  #@param\n",
        "# Uncomment the following 2 lines if you have your own CSV\n",
        "import pandas as pd\n",
        "train_df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# train_df = pd.DataFrame(load_dataset(\"hululuzhu/silly-emoji-qa\", split=[\"train\"]))\n",
        "\n",
        "# Branched from Alpaca-LoRA\n",
        "def tokenize_fn(data_point):\n",
        "  prompt_in, prompt_out = data_point['Question'], data_point['Answer']\n",
        "  full_prompt = prompt_in + prompt_out\n",
        "  tokenized_full_prompt = tokenize(tokenizer, full_prompt, MAX_LEN)\n",
        "  user_prompt = prompt_in\n",
        "  tokenized_user_prompt = tokenize(tokenizer, user_prompt, MAX_LEN, add_eos_token=False)\n",
        "  user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
        "  tokenized_full_prompt[\"labels\"] = [\n",
        "      -100 # special id for skipping\n",
        "  ] * user_prompt_len + tokenized_full_prompt[\"labels\"][user_prompt_len:]\n",
        "  return tokenized_full_prompt\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "train_ds = train_ds.flatten()\n",
        "tokenized_train_ds = train_ds.map(\n",
        "    tokenize_fn,\n",
        "    remove_columns=['Question', 'Answer'],\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9C2nsNxBOlA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample the data if you want\n",
        "# train_df.sample(3)"
      ],
      "metadata": {
        "id": "tIPWZF02Ptws"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Let's train the AI together!\n",
        "print(\"It takes about 5-10 mins, be patient\")\n",
        "trainer = transformers.Trainer(\n",
        "    model=llama,\n",
        "    train_dataset=tokenized_train_ds,\n",
        "    args=transformers.TrainingArguments(\n",
        "        # increased batch size will significantly increase GPU requirement here\n",
        "        # Decrease to 4 if you have less than 16G vram\n",
        "        # Batch = 4, probably 8.3-8.8G vram\n",
        "        # Batch = 16, 9.5G+\n",
        "        # Batch = 32, 11G+\n",
        "        # Batch = 64, 14G+\n",
        "        per_device_train_batch_size=16,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_steps=8,\n",
        "        num_train_epochs=4,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=20,\n",
        "        output_dir='outputs',\n",
        "        remove_unused_columns=False,\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
        "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True,\n",
        "    ),\n",
        ")\n",
        "llama.config.use_cache = False # Alpaca Lora sets this for training\n",
        "trainer.train()\n",
        "emoji_gpt = llama"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_LYagf_zRjm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training is complete successfully! Let's check if EmojiGPT is working?\n",
        "eval_model(emoji_gpt)"
      ],
      "metadata": {
        "id": "pPMcYKLSR4xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Optionally, upload to HuggingFace and share with the world!\n",
        "print(\"You will first need to register account at huggingface.co\")\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "YOUR_HF_ID = \"hululuzhu\"  #@param {type:\"string\"}\n",
        "YOUR_PROJECT_ID = \"emoji-gpt\"  #@param {type:\"string\"}\n",
        "\n",
        "print(\"Uploading now\")\n",
        "emoji_gpt.push_to_hub(f\"{YOUR_HF_ID}/{YOUR_PROJECT_ID}\",\n",
        "                      use_auth_token=True,\n",
        "                      create_pr=True)\n",
        "\n",
        "print(f\"Uploading complet, click \\\"Merge\\\" at https://huggingface.co/{YOUR_HF_ID}/{YOUR_PROJECT_ID}/discussions to finish publishing!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5vXqOO4ySkaR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}